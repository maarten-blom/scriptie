{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import networkx as nx\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_largest_text_block(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    \n",
    "    largest_font_size = 0\n",
    "    largest_font_text = \"\"\n",
    "\n",
    "    \n",
    "    page = document[0]\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "    for block in blocks:\n",
    "        if \"lines\" in block:\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    font_size = int(span[\"size\"])\n",
    "                    if not span[\"text\"] == \"inf\" and len(span[\"text\"]) > 10:\n",
    "                        if font_size > largest_font_size:\n",
    "                            largest_font_size = font_size\n",
    "                            largest_font_text = span[\"text\"]\n",
    "                        elif font_size == largest_font_size:\n",
    "                            largest_font_text += \" \" + span[\"text\"]\n",
    "    return largest_font_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(title):\n",
    "    time.sleep(2)\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={title}&fields=paperId,title\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'data' in data:\n",
    "            paper_id = data[\"data\"][0][\"paperId\"]\n",
    "            return paper_id\n",
    "    return response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_pdf_files(base_path):\n",
    "    df = pd.DataFrame(columns=['paper_text', 'slide_text', \"paper_name\", \"slide_name\", \"title\", \"paper_id\"])\n",
    "\n",
    "    \n",
    "    for folder_num in tqdm(range(4984)):\n",
    "        folder_path = os.path.join(base_path, str(folder_num))\n",
    "        slide_name = None\n",
    "        paper_name = None\n",
    "            \n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                if \"slide\" in file_name or \"Slide\" in file_name:\n",
    "                    slide_name = file_name\n",
    "                else:\n",
    "                    paper_name = file_name\n",
    "                    \n",
    "        slide_pdf_path = os.path.join(folder_path, slide_name)\n",
    "        paper_pdf_path = os.path.join(folder_path, paper_name)\n",
    "        \n",
    "        slide_text = extract_text_from_pdf(slide_pdf_path)\n",
    "        paper_text = extract_text_from_pdf(paper_pdf_path)\n",
    "        \n",
    "        \n",
    "        title = extract_largest_text_block(paper_pdf_path)\n",
    "        paper_id = get_id(title)\n",
    "        \n",
    "        df.loc[folder_num] = [paper_text, slide_text, paper_name, slide_name, title, paper_id]\n",
    "        \n",
    "    while (df['paper_id'] == 429).sum() > 0:\n",
    "        print((df['paper_id'] == 429).sum())\n",
    "        df.loc[df['paper_id'] == 429, 'paper_id'] = df.loc[df['paper_id'] == 429, 'title'].apply(get_id)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                         | 0/4984 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                               | 1/4984 [00:02<3:51:40,  2.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                               | 2/4984 [00:05<3:50:43,  2.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                               | 3/4984 [00:07<3:38:33,  2.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                               | 4/4984 [00:10<3:41:01,  2.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                               | 5/4984 [00:13<3:39:45,  2.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                               | 6/4984 [00:15<3:39:27,  2.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                               | 7/4984 [00:18<3:38:11,  2.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                              | 8/4984 [00:20<3:29:42,  2.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                              | 9/4984 [00:23<3:27:37,  2.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                             | 10/4984 [00:25<3:30:21,  2.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                             | 11/4984 [00:28<3:40:09,  2.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                             | 12/4984 [00:31<3:44:10,  2.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                                                             | 13/4984 [00:34<3:41:52,  2.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Define the base path of your dataset\n",
    "base_path = \"dataset\"\n",
    "\n",
    "# Process the PDF files and create the dataframe\n",
    "df = process_pdf_files(base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.lower()\n",
    "    if text:\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "df.paper_text = df.paper_text.apply(clean)\n",
    "df.slide_text = df.slide_text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_content = df.paper_text.tolist() + df.slide_text.tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(combined_content)\n",
    "\n",
    "X_papers = X[:len(df)]\n",
    "X_slides = X[len(df):]\n",
    "\n",
    "text_similarity_matrix = cosine_similarity(X_papers, X_slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_references(paper_id):\n",
    "    time.sleep(2)\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        references = [entry['citedPaper']['paperId'] for entry in data['data'] if entry['citedPaper']['paperId'] is not None]\n",
    "        return  references\n",
    "    else:\n",
    "        return response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['references'] = df['paper_id'].apply(get_references)\n",
    "\n",
    "while (df['references'] == 429).any():\n",
    "    print((df['references'] == 429).sum())\n",
    "    df.loc[df['references'] == 429, 'references'] = df.loc[df['references'] == 429, 'paper_id'].apply(get_references)\n",
    "    \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ids = set(df['paper_id'])\n",
    "\n",
    "def filter_references(ref_list):\n",
    "    if isinstance(ref_list, list):\n",
    "        return [ref for ref in ref_list if ref in valid_ids]\n",
    "    return []\n",
    "\n",
    "df['filtered_references'] = df['references'].apply(filter_references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()  # Directed graph\n",
    "\n",
    "# Create edges\n",
    "edges = [(row['paper_id'], ref) for _, row in df.iterrows() for ref in row['filtered_references']]\n",
    "\n",
    "# Add nodes and edges to the graph only if they have edges\n",
    "nodes_with_edges = set([edge[0] for edge in edges] + [edge[1] for edge in edges])\n",
    "G.add_nodes_from(nodes_with_edges)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Nodes:\", len(G.nodes()))\n",
    "print(\"Edges:\", len(G.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson R for paper and slide similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "tfidf_paper = vectorizer.fit_transform(df['paper_text'])\n",
    "cosine_sim_paper = cosine_similarity(tfidf_paper)\n",
    "\n",
    "# Calculate TFIDF for slide texts\n",
    "tfidf_slide = vectorizer.fit_transform(df['slide_text'])\n",
    "cosine_sim_slide = cosine_similarity(tfidf_slide)\n",
    "\n",
    "# Flatten the upper triangular matrices of the cosine similarity matrices\n",
    "indices = np.triu_indices_from(cosine_sim_paper, k=1)\n",
    "cosine_sim_paper_flat = cosine_sim_paper[indices]\n",
    "cosine_sim_slide_flat = cosine_sim_slide[indices]\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "correlation, p_value = pearsonr(cosine_sim_paper_flat, cosine_sim_slide_flat)\n",
    "\n",
    "# Display the results\n",
    "correlation, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Extract linked pairs from the network\n",
    "linked_pairs = list(G.edges())\n",
    "\n",
    "# Extract all pairs from the similarity matrix\n",
    "all_pairs = [(i, j) for i in range(len(df)) for j in range(i + 1, len(df))]\n",
    "\n",
    "# Create lists to store similarities\n",
    "p_linked_similarities = []\n",
    "p_non_linked_similarities = []\n",
    "\n",
    "# Populate the lists with cosine similarities\n",
    "for i, j in tqdm(all_pairs):\n",
    "    similarity = cosine_sim_slide[i, j]\n",
    "    if (df['paper_id'].iloc[i], df['paper_id'].iloc[j]) in linked_pairs or (df['paper_id'].iloc[j], df['paper_id'].iloc[i]) in linked_pairs:\n",
    "        p_linked_similarities.append(similarity)\n",
    "    else:\n",
    "        p_non_linked_similarities.append(similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_linked_average = np.mean(p_linked_similarities)\n",
    "p_linked_std = np.std(p_linked_similarities)\n",
    "\n",
    "p_non_linked_average = np.mean(p_non_linked_similarities)\n",
    "p_non_linked_std = np.std(p_non_linked_similarities)\n",
    "\n",
    "print(\"Linked Similarities - Average:\", p_linked_average, \"Standard Deviation:\", p_linked_std)\n",
    "print(\"Non-Linked Similarities - Average:\", p_non_linked_average, \"Standard Deviation:\", p_non_linked_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a statistical test to check if there's a significant difference\n",
    "t_stat, p_value = ttest_ind(p_linked_similarities, p_non_linked_similarities)\n",
    "\n",
    "# Display the results\n",
    "t_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the non-linked histogram on ax1 (left y-axis)\n",
    "ax1.hist(p_non_linked_similarities, bins=30, alpha=0.5, label='Non-Linked Similarities', color='red')\n",
    "ax1.set_xlabel('Similarity')\n",
    "ax1.set_ylabel('Frequency (Non-Linked)', color='red')\n",
    "ax1.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Create a second y-axis sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.hist(p_linked_similarities, bins=30, alpha=0.5, label='Linked Similarities', color='blue')\n",
    "ax2.set_ylabel('Frequency (Linked)', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Add a title\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides Similarity Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract linked pairs from the network\n",
    "linked_pairs = list(G.edges())\n",
    "\n",
    "# Extract all pairs from the similarity matrix\n",
    "all_pairs = [(i, j) for i in range(len(df)) for j in range(i + 1, len(df))]\n",
    "\n",
    "# Create lists to store similarities\n",
    "s_linked_similarities = []\n",
    "s_non_linked_similarities = []\n",
    "\n",
    "# Populate the lists with cosine similarities\n",
    "for i, j in tqdm(all_pairs):\n",
    "    similarity = cosine_sim_paper[i, j]\n",
    "    if (df['paper_id'].iloc[i], df['paper_id'].iloc[j]) in linked_pairs or (df['paper_id'].iloc[j], df['paper_id'].iloc[i]) in linked_pairs:\n",
    "        s_linked_similarities.append(similarity)\n",
    "    else:\n",
    "        s_non_linked_similarities.append(similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_linked_average = np.mean(s_linked_similarities)\n",
    "s_linked_std = np.std(s_linked_similarities)\n",
    "\n",
    "s_non_linked_average = np.mean(s_non_linked_similarities)\n",
    "s_non_linked_std = np.std(s_non_linked_similarities)\n",
    "\n",
    "print(\"Linked Similarities - Average:\", s_linked_average, \"Standard Deviation:\", s_linked_std)\n",
    "print(\"Non-Linked Similarities - Average:\", s_non_linked_average, \"Standard Deviation:\", s_non_linked_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a statistical test to check if there's a significant difference\n",
    "t_stat, p_value = ttest_ind(s_linked_similarities, s_non_linked_similarities)\n",
    "\n",
    "# Display the results\n",
    "t_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the non-linked histogram on ax1 (left y-axis)\n",
    "ax1.hist(s_non_linked_similarities, bins=30, alpha=0.5, label='Non-Linked Similarities', color='red')\n",
    "ax1.set_xlabel('Similarity')\n",
    "ax1.set_ylabel('Frequency (Non-Linked)', color='red')\n",
    "ax1.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Create a second y-axis sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.hist(s_linked_similarities, bins=30, alpha=0.5, label='Linked Similarities', color='blue')\n",
    "ax2.set_ylabel('Frequency (Linked)', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Add a title\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))  # Increase the size of the image\n",
    "nx.draw(G, with_labels=False, node_size=10, width=0.1)\n",
    "plt.title(\"Network Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
